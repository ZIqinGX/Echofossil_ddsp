import os
import sys
import numpy as np
import librosa
from scipy.io.wavfile import write as write_wav

import gin
import ddsp
import ddsp.training
from ddsp.training.models import Autoencoder
from ddsp.spectral_ops import compute_f0, compute_loudness
from pydub import AudioSegment
from pydub.generators import WhiteNoise

def save_audio(path, audio, sample_rate):
    """æŠŠæµ®ç‚¹[-1,1]çš„éŸ³é¢‘ä¿å­˜æˆ 16-bit WAV."""
    # é˜²æ­¢æº¢å‡º
    audio = np.clip(audio, -1.0, 1.0)
    # è½¬ int16
    audio_int16 = (audio * 32767).astype(np.int16)
    write_wav(path, sample_rate, audio_int16)

# === è·¯å¾„è®¾ç½® ===
GIN_FILE   = r'C:\Users\OS\Desktop\MusicRecordingTransfeer\ddsp_test\models\flute\operative_config-0.gin'
CKPT_FILE  = r'C:\Users\OS\Desktop\MusicRecordingTransfeer\ddsp_test\models\flute\ckpt-20000'
INPUT_WAV  = r'C:\Users\OS\Desktop\MusicRecordingTransfeer\ddsp_test\audio\input2.wav'
OUTPUT_WAV = r'C:\Users\OS\Desktop\MusicRecordingTransfeer\ddsp_test\output_flute\output2.wav'

# === è¶…å‚ ===
TARGET_SR  = 16000
FRAME_RATE = 250

# è§£æé…ç½®æ—¶è·³è¿‡é‚£äº›ä¸å­˜åœ¨çš„ç»‘å®šï¼ˆSoloFluteã€train.* ç­‰ï¼‰
gin.parse_config_file(GIN_FILE, skip_unknown=True)

# 1. è¯»å¹¶é‡é‡‡æ ·
audio, sr = librosa.load(INPUT_WAV, sr=TARGET_SR, mono=True)
print(f"ğŸ§ åŠ è½½éŸ³é¢‘: {INPUT_WAV} é‡‡æ ·ç‡={sr} æ—¶é•¿={len(audio)/sr:.2f}s")

# 2. ç‰¹å¾æå–ï¼ˆæ‰‹åŠ¨åšé¢„å¤„ç†ï¼‰
f0_hz, f0_conf   = compute_f0(audio, FRAME_RATE)
loudness_db      = compute_loudness(audio, FRAME_RATE)
n = min(len(f0_hz), len(loudness_db))
f0_hz, f0_conf  = f0_hz[:n], f0_conf[:n]
loudness_db     = loudness_db[:n]
features = {
    'f0_hz': f0_hz[np.newaxis, :],
    'f0_confidence': f0_conf[np.newaxis, :],
    'loudness_db': loudness_db[np.newaxis, :]
}
print(f"ğŸ“ˆ ç‰¹å¾å¸§æ•°: {n}")

# 3. åŠ è½½æ¨¡å‹
gin.parse_config_file(GIN_FILE)
model = Autoencoder()
model.restore(CKPT_FILE)
print(f"âœ… æ¨¡å‹åŠ è½½: {CKPT_FILE}")

# 4. æ¨ç†åˆæˆ
# æŠŠ audio å’Œ ç‰¹å¾ åˆå¹¶åˆ°åŒä¸€ä¸ªè¾“å…¥ dict
# 4. æ¨ç†åˆæˆ
model_input = {
    'audio': audio[np.newaxis, :],
    'f0_hz': f0_hz[np.newaxis, :],
    'f0_confidence': f0_conf[np.newaxis, :],
    'loudness_db': loudness_db[np.newaxis, :]
}
outputs = model(model_input, training=False)

# å…¼å®¹å• Tensor æˆ– dict
if isinstance(outputs, dict):
    audio_tensor = outputs['audio_synth']
else:
    audio_tensor = outputs

audio_out = audio_tensor.numpy().flatten()

# 5. ä¿å­˜
save_audio(OUTPUT_WAV, audio_out, TARGET_SR)

# 6.æ··å“
# è¯»å–è¾“å‡ºçš„ .wav æ–‡ä»¶
audio = AudioSegment.from_wav(OUTPUT_WAV)
# æ¨¡æ‹Ÿç©ºé—´æ„Ÿçš„æ··å“ï¼šå°¾éƒ¨è¡°å‡ + å»¶è¿Ÿå åŠ 
def add_reverb_delay(audio, reverb_tail_ms=400, delay_ms=100, decay=0.4, repeats=4):
    # åˆ›å»ºå»¶è¿Ÿå›å£°è½¨é“
    echo = audio
    for i in range(1, repeats + 1):
        attenuated = audio - (i * decay * 10)  # è¡°å‡
        echo = echo.overlay(attenuated, position=i * delay_ms)

    # æ·¡å…¥æ·¡å‡º + å°¾éƒ¨é™éŸ³å»¶ä¼¸
    reverbed = echo.fade_in(300).fade_out(reverb_tail_ms)
    return reverbed + AudioSegment.silent(duration=reverb_tail_ms)


# æ·»åŠ æ•ˆæœ
processed_audio = add_reverb_delay(audio)

# ä¿å­˜å¤„ç†åçš„éŸ³é¢‘
processed_path = OUTPUT_WAV.replace('.wav', '_reverb.wav')
processed_audio.export(processed_path, format='wav')
print(f"âœ¨ å·²ä¿å­˜å¸¦ç©ºé—´æ„Ÿçš„ç‰ˆæœ¬ï¼š{processed_path}")

from pydub import AudioSegment
import random

# åŠ è½½ä¿å­˜å¥½çš„ DDSP åˆæˆéŸ³é¢‘
original = AudioSegment.from_wav(OUTPUT_WAV)

# 3. å®šä¹‰ granular_echo å‡½æ•°
def granular_echo(audio, grain_ms=80, repeat=8, max_delay_ms=300):
    new_audio = audio
    for i in range(repeat):
        offset = random.randint(10, max_delay_ms)
        grain = audio[:grain_ms] - (i * 6)
        new_audio = new_audio.overlay(grain, position=offset * i)
    return new_audio

# 4. åº”ç”¨ granular æ•ˆæœ
granular_version = granular_echo(original)

# 5. ä¿å­˜å¸¦æ•ˆæœçš„ç‰ˆæœ¬
granular_path = OUTPUT_WAV.replace('.wav', '_fossilscape.wav')
granular_version.export(granular_path, format='wav')
print(f"âœ¨ å·²ä¿å­˜å¸¦é¢—ç²’æ„Ÿè¿œå¤å£°éŸ³ç‰ˆæœ¬ï¼š{granular_path}")